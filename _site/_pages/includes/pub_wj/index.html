<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Wanjun Zhong (ÈíüÂÆõÂêõ) - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Wanjun Zhong (ÈíüÂÆõÂêõ)">
<meta property="og:title" content="Wanjun Zhong (ÈíüÂÆõÂêõ)">


  <link rel="canonical" href="http://0.0.0.0:4000/_pages/includes/pub_wj/">
  <meta property="og:url" content="http://0.0.0.0:4000/_pages/includes/pub_wj/">











<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
<link rel="manifest" href="/images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-publications">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="/#-educations">Educations</a></li>
          
            <li class="masthead__menu-item"><a href="/#-invited-talks">Invited Talks</a></li>
          
            <li class="masthead__menu-item"><a href="/#-internships">Internships</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="/images/wanjun_profile.jpg" class="author__avatar" alt="Wanjun Zhong (ÈíüÂÆõÂêõ)">
  </div>

  <div class="author__content">
    <h3 class="author__name">Wanjun Zhong (ÈíüÂÆõÂêõ)</h3>
    <p class="author__bio">ByteDance Seed</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;">Senior Research Scientist at ByteDance Seed, focusing on Large Language Models, Reasoning towards AGI, and Agent Foundation Models.</div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> China</li>
      
      
      
      
        <li><a href="mailto:wanjun@bytedance.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/zhongwanjun"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=FGIZfyQAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:wanjun@bytedance.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
      
      
      
      
      
      
        <a href="https://github.com/zhongwanjun"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?user=FGIZfyQAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="üìù Publications">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            
<h1 id="-publications">üìù Publications</h1>
<h2 id="works-in-seed">Works in Seed</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Seed Technical Report</code> <a href="https://arxiv.org/pdf/2505.07062">Seed1.5-VL Technical Report</a></li>
  <li><code class="language-plaintext highlighter-rouge">Seed Technical Report</code> <a href="https://arxiv.org/pdf/2504.13914v1">Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning</a></li>
  <li><code class="language-plaintext highlighter-rouge">Seed Technical Report</code> <a href="https://arxiv.org/abs/2501.12326">UI-TARS: Pioneering Automated GUI Interaction with Native Agents</a></li>
  <li><code class="language-plaintext highlighter-rouge">Sub. to NeurIPS 2025</code> <a href="https://arxiv.org/pdf/2504.11536?">Retool: Reinforcement learning for strategic tool use in llms</a>, Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, <strong>Wanjun Zhong</strong></li>
  <li><code class="language-plaintext highlighter-rouge">Arxiv</code> <a href="https://arxiv.org/pdf/2410.20424?">Autokaggle: A multi-agent framework for autonomous data science competitions</a>, Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, <strong>Wanjun Zhong</strong>, Wangchunshu Zhou, Wenhao Huang, Ge Zhang</li>
  <li><code class="language-plaintext highlighter-rouge">EMNLP 2025</code> <a href="https://arxiv.org/pdf/2504.14870?">Otc: Optimal tool calls via reinforcement learning</a>, Hongru Wang, Cheng Qian, <strong>Wanjun Zhong</strong>, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, Heng Ji</li>
</ul>

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2019</div><img src='images/fs.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[FastSpeech: Fast, Robust and Controllable Text to Speech](https://papers.nips.cc/paper/8580-fastspeech-fast-robust-and-controllable-text-to-speech.pdf) \\
**Yi Ren**, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu

[**Project**](https://speechresearch.github.io/fastspeech/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>

- FastSpeech is the first fully parallel end-to-end speech synthesis model.
- **Academic Impact**: This work is included by many famous speech synthesis open-source projects, such as [ESPNet ![](https://img.shields.io/github/stars/espnet/espnet?style=social)](https://github.com/espnet/espnet). Our work are promoted by more than 20 media and forums, such as [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/UkFadiUBy-Ymn-zhJ95JcQ)„ÄÅ[InfoQ](https://www.infoq.cn/article/tvy7hnin8bjvlm6g0myu).
- **Industry Impact**: FastSpeech has been deployed in [Microsoft Azure TTS service](https://techcommunity.microsoft.com/t5/azure-ai/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911) and supports 49 more languages with state-of-the-art AI quality. It was also shown as a text-to-speech system acceleration example in [NVIDIA GTC2020](https://resources.nvidia.com/events/GTC2020s21420).
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2021</div><img src='images/fs2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[FastSpeech 2: Fast and High-Quality End-to-End Text to Speech](https://arxiv.org/abs/2006.04558) \\
**Yi Ren**, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu

[**Project**](https://speechresearch.github.io/fastspeech2/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:LkGwnXOMwfcC'></span></strong>
  - This work is included by many famous speech synthesis open-source projects, such as [PaddlePaddle/Parakeet ![](https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?style=social)](https://github.com/PaddlePaddle/PaddleSpeech), [ESPNet ![](https://img.shields.io/github/stars/espnet/espnet?style=social)](https://github.com/espnet/espnet) and [fairseq ![](https://img.shields.io/github/stars/pytorch/fairseq?style=social)](https://github.com/pytorch/fairseq).
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/mega.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis](https://openreview.net/forum?id=mvMI3N4AvD) \\ 
Ziyue Jiang, Jinglin Liu, **Yi Ren**, et al.

[**Project**](https://boostprompt.github.io/boostprompt/) 
  - This work has been deployed on many TikTok products.
  - Advandced zero-shot voice cloning model.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2022</div><img src='images/diffsinger.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism](https://arxiv.org/abs/2105.02446) \\
Jinglin Liu, Chengxi Li, **Yi Ren**, Feiyang Chen, Zhou Zhao

- Many [video demos](https://www.bilibili.com/video/BV1be411N7JA) created by the [DiffSinger community](https://github.com/openvpi) are released.
- DiffSinger was introduced in [a very popular video](https://www.bilibili.com/video/BV1uM411t7ZJ) (1600k+ views) on Bilibili!

- [**Project**](https://diffsinger.github.io/) \| [![](https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=DiffSpeech Stars)](https://github.com/NATSpeech/NATSpeech) \| [![](https://img.shields.io/github/stars/MoonInTheRiver/DiffSinger?style=social&label=DiffSinger Stars)](https://github.com/MoonInTheRiver/DiffSinger) \| [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo)](https://huggingface.co/spaces/NATSpeech/DiffSpeech)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2021</div><img src='images/portaspeech.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[PortaSpeech: Portable and High-Quality Generative Text-to-Speech](https://arxiv.org/abs/2109.15166) \\
**Yi Ren**, Jinglin Liu, Zhou Zhao

[**Project**](https://portaspeech.github.io/) \| [![](https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=Code+Stars)](https://github.com/NATSpeech/NATSpeech) \| [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo)](https://huggingface.co/spaces/NATSpeech/PortaSpeech)
</div>
</div> -->

<h2 id="large-language-model-reasoning">Large Language Model Reasoning</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Seed Technical Report</code> <a href="https://arxiv.org/pdf/2504.13914v1">Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning</a></li>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2025</code> <a href="https://arxiv.org/abs/2312.11370">G-llava: Solving geometric problem with multi-modal large language model</a>, Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, <strong>Wanjun Zhong</strong>, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, Lingpeng Kong</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2025</code> <a href="https://arxiv.org/pdf/2505.14116">Self-reasoning language models: Unfold hidden reasoning chains with few reasoning catalyst</a>, Hongru Wang, Deng Cai, <strong>Wanjun Zhong</strong>, Shijue Huang, Jeff Z Pan, Zeming Liu, Kam-Fai Wong</li>
  <li><code class="language-plaintext highlighter-rouge">Information Processing &amp; Management</code> <a href="https://arxiv.org/pdf/2310.01446">Adaptive-solver framework for dynamic strategy selection in large language model reasoning</a>, Jianpeng Zhou, <strong>Wanjun Zhong</strong>, Yanlin Wang, Jiahai Wang</li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2025</code> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/35164/37319">Exploring iterative enhancement for improving learnersourced multiple-choice question explanations with large language models</a>, Qiming Bao, Juho Leinonen, Alex Yuxuan Peng, <strong>Wanjun Zhong</strong>, Ga√´l Gendron, Timothy Pistotti, Alice Huang, Paul Denny, Michael Witbrock, Jiamou Liu</li>
</ul>

<h2 id="general-agent-model--system">General Agent Model &amp; System</h2>
<h3 id="multi-modal-agent-gui-etc">Multi-modal Agent (GUI etc.)</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Seed Technical Report</code> <a href="https://arxiv.org/abs/2501.12326">UI-TARS: Pioneering Automated GUI Interaction with Native Agents</a>, Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, <strong>Wanjun Zhong</strong>, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, Guang Shi</li>
</ul>

<h3 id="tool-learning-agent">Tool-Learning Agent</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Sub. to NeurIPS 2025</code> <a href="https://arxiv.org/pdf/2504.11536?">Retool: Reinforcement learning for strategic tool use in llms</a>, Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, <strong>Wanjun Zhong</strong></li>
  <li><code class="language-plaintext highlighter-rouge">Arxiv</code> <a href="https://arxiv.org/pdf/2410.20424?">Autokaggle: A multi-agent framework for autonomous data science competitions</a>, Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, <strong>Wanjun Zhong</strong>, Wangchunshu Zhou, Wenhao Huang, Ge Zhang</li>
  <li><code class="language-plaintext highlighter-rouge">EMNLP 2025</code> <a href="https://arxiv.org/pdf/2504.14870?">Otc: Optimal tool calls via reinforcement learning</a>, Hongru Wang, Cheng Qian, <strong>Wanjun Zhong</strong>, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, Heng Ji</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2024</code> <a href="https://arxiv.org/pdf/2401.15670">Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios</a>, Shijue Huang, <strong>Wanjun Zhong</strong>, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng Xu, Qun Liu</li>
</ul>

<h3 id="code-agent">Code Agent</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Arxiv</code> <a href="https://arxiv.org/pdf/2409.09030?">Agents in software engineering: Survey, landscape, and vision</a>, Yanlin Wang, <strong>Wanjun Zhong</strong>, Yanxian Huang, Ensheng Shi, Min Yang, Jiachi Chen, Hui Li, Yuchi Ma, Qianxiang Wang, Zibin Zheng</li>
  <li><code class="language-plaintext highlighter-rouge">ICSME 2023</code> <a href="https://arxiv.org/pdf/2208.03229.pdf">You Augment Me: Exploring ChatGPT-based Data Augmentation for Semantic Code Search</a>, Yanlin Wang, Lianghong Guo, Ensheng Shi, Wenqing Chen, Jiachi Chen, <strong>Wanjun Zhong</strong>, Menghan Wang, Hui Li, Hongyu Zhang, Ziyu Lyu, Zibin Zheng</li>
  <li><code class="language-plaintext highlighter-rouge">ISSTA 24 - Outstanding Paper Award</code> <a href="https://arxiv.org/pdf/2407.20042">When to stop? towards efficient code generation in llms with excess token prevention</a>, Lianghong Guo, Yanlin Wang, Ensheng Shi, <strong>Wanjun Zhong</strong>, Hongyu Zhang, Jiachi Chen, Ruikai Zhang, Yuchi Ma, Zibin Zheng</li>
</ul>

<h3 id="agent-memory">Agent Memory</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2024</code> <a href="https://arxiv.org/abs/2305.10250">MemoryBank: Enhancing Large Language Models with Long-Term Memory</a>, <strong>Wanjun Zhong</strong>, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang</li>
</ul>

<h3 id="agent-driven-training">Agent-driven Training</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">arXiv 2024</code> <a href="https://arxiv.org/pdf/2401.15670">YODA: Teacher-Student Progressive Learning for Language Models</a>, Jianqiao Lu<em>, <strong>Wanjun Zhong</strong></em>, Yufei Wang, Zhijiang Guo, Qi Zhu, Wenyong Huang, Yanlin Wang, Fei Mi, Baojun Wang, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu (* equal contribution)</li>
</ul>

<h2 id="benchmark-and-evaluation">Benchmark and Evaluation</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">NAACL 2024</code> <a href="https://arxiv.org/abs/2304.06364">AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models</a>, <strong>Wanjun Zhong</strong>, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan</li>
  <li><code class="language-plaintext highlighter-rouge">EMNLP 2024</code> <a href="https://arxiv.org/abs/2403.03514">CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models</a>, Zexuan Qiu, Jingjing Li, Shijue Huang, <strong>Wanjun Zhong</strong>, Irwin King</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2024 Workshop</code> <a href="https://arxiv.org/abs/2402.16288">PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering</a>, Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, <strong>Wanjun Zhong</strong>, Zezhong Wang, Kam-Fai Wong</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2024</code> <a href="https://arxiv.org/abs/2310.20410">Followbench: A multi-level fine-grained constraints following benchmark for large language models</a>, Yuxin Jiang, Yufei Wang, Xingshan Zeng, <strong>Wanjun Zhong</strong>, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, Wei Wang</li>
</ul>

<h2 id="self-learning-of-llms">Self-Learning of LLMs</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2025</code> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/34590/36745">Empowering Self-Learning of LLMs: Inner Knowledge Explicitation as a Catalyst</a>, Shijue Huang, <strong>Wanjun Zhong</strong>, Deng Cai, Fanqi Wan, Chengyi Wang, Mingxuan Wang, Mu Qiao, Ruifeng Xu</li>
  <li><code class="language-plaintext highlighter-rouge">arXiv 2023</code> <a href="https://arxiv.org/abs/2403.03514">SELF: Language-driven self-evolution for large language model</a>, Jianqiao Lu, <strong>Wanjun Zhong</strong>, Wenyong Huang, Yufei Wang, Fei Mi, Baojun Wang, Weichao Wang, Lifeng Shang, Qun Liu</li>
</ul>

<h2 id="general-llm-training">General LLM Training</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">arXiv 2023</code> <a href="https://arxiv.org/abs/2312.01700">Data management for large language models: A survey</a>, Zige Wang, <strong>Wanjun Zhong</strong>, Yufei Wang, Qi Zhu, Fei Mi, Baojun Wang, Lifeng Shang, Xin Jiang, Qun Liu</li>
  <li><code class="language-plaintext highlighter-rouge">arXiv 2023</code> <a href="https://arxiv.org/abs/2307.12966">Aligning large language models with human: A survey</a>, Yufei Wang, <strong>Wanjun Zhong</strong>, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2024</code> <a href="https://arxiv.org/pdf/2402.11905">Learning to Edit: Aligning LLMs with Knowledge Editing</a>, Yuxin Jiang, Yufei Wang, Chuhan Wu, <strong>Wanjun Zhong</strong>, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, Qun Liu, Wei Wang</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">ICSME 2023</code> <a href="https://arxiv.org/pdf/2208.03229.pdf">You Augment Me: Exploring ChatGPT-based Data Augmentation for Semantic Code Search</a>, Yanlin Wang, Lianghong Guo, Ensheng Shi, Wenqing Chen, Jiachi Chen, <strong>Wanjun Zhong</strong>, Menghan Wang, Hui Li, Hongyu Zhang, Ziyu Lyu, Zibin Zheng</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="https://arxiv.org/pdf/2209.10918.pdf">CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding</a>, Zhijian Hou<em>, <strong>Wanjun Zhong</strong></em>, Leiji, Kun Yan, Difei Gao, Wing-Kwong Chan, Chong-Wah Ngo, Zheng Shou, Nan Duan (* equal contribution)</li>
</ul>

<h2 id="previous-work-before-2023">Previous Work Before 2023</h2>
<h3 id="multi-modal">Multi-Modal</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ICME 2023</code> <a href="https://arxiv.org/pdf/2009.10297.pdf">Semantic Composition and Alignment with Cross-Modality-Aware Syntactic Hypergraph Convolutional Network for Video Question Answering</a>, Zenan Xu<em>, <strong>Wanjun Zhong</strong></em>, Qinliang Su, Zijing Ou, Fuwei Zhang (* equal contribution)</li>
  <li><code class="language-plaintext highlighter-rouge">ECCV Challenge 2022</code> <a href="https://arxiv.org/abs/2210.05197.pdf">An Efficient COarse-to-fiNE Alignment Framework @ Ego4D Natural Language Queries Challenge 2022</a>, Zhijian Hou<em>, <strong>Wanjun Zhong</strong></em>, Leiji, Kun Yan, Difei Gao, Wing-Kwong Chan, Chong-Wah Ngo, Zheng Shou, Nan Duan (* equal contribution) <a href="https://github.com/Jun-jie-Huang/OTTeR">[code]</a></li>
</ul>

<h3 id="knowledge-enhanced-language-model-reasoning">Knowledge-enhanced Language Model Reasoning</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ACL 2022</code> <a href="https://arxiv.org/pdf/2210.11265.pdf">Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers</a>, <strong>Wanjun Zhong</strong><em>, Tingting Ma</em>, Jiahai Wang, Jian Yin, Tiejun Zhao, Chin-Yew Lin, Nan Duan (* equal contribution)</li>
  <li><code class="language-plaintext highlighter-rouge">EMNLP 2022</code> <a href="https://arxiv.org/abs/2210.05197.pdf">Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA</a>, JunJie Huang, <strong>Wanjun Zhong</strong><em>, Qian Liu, Ming Gong, Daxin Jiang, Nan Duan (</em> equal contribution) <a href="https://github.com/Jun-jie-Huang/OTTeR">[code]</a></li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="https://arxiv.org/pdf/2205.08794.pdf">LogiGAN: Learning Logical Reasoning via Adversarial Pre-training</a>, Xinyu Pi<em>, <strong>Wanjun Zhong</strong></em>, Yan Gao, Jian-guang Lou, Nan Duan (* equal contribution) <a href="https://github.com/microsoft/ContextualSP/tree/master/logigan">[code]</a></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022 (Oral)</code> <a href="https://arxiv.org/pdf/2201.05880.pdf">Reasoning over Hybrid Chain for Table-and-Text Open Domain Question Answering</a>, <strong>Wanjun Zhong</strong><em>, Junjie Huang</em>, Qian Liu, Ming Zhou, Jiahai Wang, Jian Yin, Nan Duan (* equal contribution) <a href="https://github.com/zhongwanjun/CARP">[code]</a></li>
  <li><code class="language-plaintext highlighter-rouge">NAACL 2022</code> <a href="https://openreview.net/pdf?id=g68eYTS0rzJ">ProQA: Structural Prompt-based Pre-training for Unified Question Answering</a>, <strong>Wanjun Zhong</strong><em>, Yifan Gao, Ning Ding, Yujia Qin, Zhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin, Nan Duan (</em> equal contribution) <a href="https://github.com/zhongwanjun/ProQA">[code]</a></li>
  <li><code class="language-plaintext highlighter-rouge">TASLP 2022</code> <a href="https://arxiv.org/abs/2108.00648">From LSAT: The Progress and Challenges of Complex Reasoning</a>, Siyuan Wang, Zhongkun Liu, <strong>Wanjun Zhong</strong>, Ming Zhou, Zhongyu Wei, Zhumin Chen, Nan Duan</li>
  <li><code class="language-plaintext highlighter-rouge">NAACL 2022</code> <a href="https://arxiv.org/pdf/2104.06598.pdf">Analytical Reasoning of Text</a>, <strong>Wanjun Zhong</strong>, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, Nan Duan <a href="https://github.com/zhongwanjun/AR-LSAT">[code]</a></li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2022</code> <a href="https://arxiv.org/pdf/2004.03070.pdf">Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text</a>, Siyuan Wang, <strong>Wanjun Zhong</strong>, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, Nan Duan <a href="https://github.com/WangsyGit/LReasoner">[code]</a><br />
Here is the formatted list in the requested style:</li>
  <li><code class="language-plaintext highlighter-rouge">EMNLP 2021</code> <a href="https://arxiv.org/abs/2104.01767">WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach</a>, Junjie Huang, Duyu Tang, <strong>Wanjun Zhong</strong>, Shuai Lu, Linjun Shou, Ming Gong, Daxin Jiang, Nan Duan</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2021</code> <a href="https://aclanthology.org/2021.findings-acl.129.pdf">UserAdapter: Few-Shot User Learning in Sentiment Analysis</a>, <strong>Wanjun Zhong</strong>, Duyu Tang, Jiahai Wang, Jian Yin, Nan Duan</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2021</code> <a href="https://arxiv.org/pdf/2012.14116.pdf">Syntax-Enhanced Pre-trained Model</a>, Zenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun Shou, Ming Gong, <strong>Wanjun Zhong</strong>, Xiaojun Quan, Daxin Jiang, Nan Duan</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2021</code> <a href="">Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge</a>, Linmei Hu, Tianchi Yang, Luhao Zhang, <strong>Wanjun Zhong</strong>, Duyu Tang, Chuan Shi, Nan Duan, Ming Zhou</li>
  <li><code class="language-plaintext highlighter-rouge">EMNLP 2020 (Oral)</code> <a href="https://arxiv.org/pdf/2010.07475.pdf">Neural Deepfake Detection with Factual Structure of Text</a>, <strong>Wanjun Zhong</strong>, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin <a href="https://slideslive.com/38938858/neural-deepfake-detection-with-factual-structure-of-text">[video]</a></li>
  <li><code class="language-plaintext highlighter-rouge">EMNLP 2020</code> <a href="https://arxiv.org/pdf/2004.14201.pdf">Leveraging declarative knowledge in text and first-order logic for fine-grained propaganda detection</a>, Ruize Wang, Duyu Tang, Nan Duan, <strong>Wanjun Zhong</strong>, Zhongyu Wei, Xuanjing Huang, Daxin Jiang, Ming Zhou</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2020</code> <a href="https://www.aclweb.org/anthology/2020.acl-main.539.pdf">LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network</a>, <strong>Wanjun Zhong</strong>, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang, Jiahai Wang, Jian Yin <a href="https://slideslive.com/38928864/logicalfactchecker-leveraging-logical-operations-for-fact-checking-with-graph-module-network">[video]</a></li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2020</code> <a href="https://www.aclweb.org/anthology/2020.acl-main.549.pdf">Reasoning Over Semantic-Level Graph for Fact Checking</a>, <strong>Wanjun Zhong</strong>, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin <a href="https://slideslive.com/38928866/reasoning-over-semanticlevel-graph-for-fact-checking">[video]</a></li>
  <li><code class="language-plaintext highlighter-rouge">NLPCC 2019</code> <a href="https://arxiv.org/pdf/1809.03568.pdf">Improving Question Answering by Commonsense-Based Pre-Training</a>, <strong>Wanjun Zhong</strong>, Duyu Tang, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin</li>
  <li><code class="language-plaintext highlighter-rouge">AI Open 2023</code> <a href="https://arxiv.org/pdf/2208.03229.pdf">Improving Task Generalization via Unified Schema Prompt</a>, <strong>Wanjun Zhong</strong>, Yifan Gao, Ning Ding, Zhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin, Nan Duan</li>
  <li><code class="language-plaintext highlighter-rouge">arXiv 2020</code> <a href="https://arxiv.org/pdf/2009.10297.pdf">A Heterogeneous Graph with Factual, Temporal and Logical Knowledge for Question Answering Over Dynamic Contexts</a>, <strong>Wanjun Zhong</strong>, Duyu Tang, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin</li>
</ul>


          </section>
        </div>
      </article>
    </div>

    <script src="/assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/zhongwanjun/wanjun.github.io@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            // var totalCitation = data['citedby']
            // document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
